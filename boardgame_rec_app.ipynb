{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from time import sleep\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request(msg, slp=1):\n",
    "    status_code = 500  # Want to get a status-code of 200\n",
    "    while status_code != 200:\n",
    "        sleep(slp)  # Don't ping the server too often\n",
    "        try:\n",
    "            r = requests.get(msg)\n",
    "            status_code = r.status_code\n",
    "            if status_code != 200:\n",
    "                print(\"Server Error! Response Code %i. Retrying...\" % (r.status_code))\n",
    "        except:\n",
    "            print(\"An exception has occurred, probably a momentory loss of connection. Waiting one seconds...\")\n",
    "            sleep(1)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 scraped, minimum number of ratings was 21869\n",
      "Page 2 scraped, minimum number of ratings was 14115\n",
      "Page 3 scraped, minimum number of ratings was 10474\n",
      "Page 4 scraped, minimum number of ratings was 7914\n",
      "Page 5 scraped, minimum number of ratings was 6720\n",
      "Page 6 scraped, minimum number of ratings was 5523\n",
      "Page 7 scraped, minimum number of ratings was 4758\n",
      "Page 8 scraped, minimum number of ratings was 4175\n",
      "Page 9 scraped, minimum number of ratings was 3680\n",
      "Page 10 scraped, minimum number of ratings was 3293\n",
      "Page 11 scraped, minimum number of ratings was 3000\n",
      "Page 12 scraped, minimum number of ratings was 2744\n",
      "Page 13 scraped, minimum number of ratings was 2524\n",
      "Page 14 scraped, minimum number of ratings was 2356\n",
      "Page 15 scraped, minimum number of ratings was 2164\n",
      "Page 16 scraped, minimum number of ratings was 2007\n",
      "Page 17 scraped, minimum number of ratings was 1863\n",
      "Page 18 scraped, minimum number of ratings was 1769\n",
      "Page 19 scraped, minimum number of ratings was 1660\n",
      "Page 20 scraped, minimum number of ratings was 1577\n",
      "Page 21 scraped, minimum number of ratings was 1494\n",
      "Page 22 scraped, minimum number of ratings was 1418\n",
      "Page 23 scraped, minimum number of ratings was 1357\n",
      "Page 24 scraped, minimum number of ratings was 1281\n",
      "Page 25 scraped, minimum number of ratings was 1218\n",
      "Page 26 scraped, minimum number of ratings was 1153\n",
      "Page 27 scraped, minimum number of ratings was 1103\n",
      "Page 28 scraped, minimum number of ratings was 1057\n",
      "Page 29 scraped, minimum number of ratings was 1012\n",
      "Page 30 scraped, minimum number of ratings was 976\n",
      "Page 31 scraped, minimum number of ratings was 937\n",
      "Page 32 scraped, minimum number of ratings was 901\n",
      "Page 33 scraped, minimum number of ratings was 864\n",
      "Page 34 scraped, minimum number of ratings was 832\n",
      "Page 35 scraped, minimum number of ratings was 802\n",
      "Page 36 scraped, minimum number of ratings was 771\n",
      "Page 37 scraped, minimum number of ratings was 742\n",
      "Page 38 scraped, minimum number of ratings was 716\n",
      "Page 39 scraped, minimum number of ratings was 688\n",
      "Page 40 scraped, minimum number of ratings was 666\n",
      "Page 41 scraped, minimum number of ratings was 647\n",
      "Page 42 scraped, minimum number of ratings was 622\n",
      "Page 43 scraped, minimum number of ratings was 599\n",
      "Page 44 scraped, minimum number of ratings was 581\n",
      "Page 45 scraped, minimum number of ratings was 564\n",
      "Page 46 scraped, minimum number of ratings was 548\n",
      "Page 47 scraped, minimum number of ratings was 530\n",
      "Page 48 scraped, minimum number of ratings was 516\n",
      "Page 49 scraped, minimum number of ratings was 503\n",
      "Page 50 scraped, minimum number of ratings was 490\n"
     ]
    }
   ],
   "source": [
    "# Initialize a DF to hold all our scraped game info\n",
    "df_all = pd.DataFrame(columns=[\"id\", \"name\", \"nrate\", \"pic_url\"])\n",
    "min_nrate = 1e5\n",
    "npage = 1\n",
    "\n",
    "# Scraping successful pages in the results until we get down to games with < 500 ratings each\n",
    "while min_nrate > 500:\n",
    "    # Get full HTML for a specific page in the full listing of boardgames sorted by nrates \n",
    "    r = request(\"https://boardgamegeek.com/browse/boardgame/page/%i?sort=numvoters&sortdir=desc\" % (npage,))\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")    \n",
    "    \n",
    "    # Get rows for the table listing all the games on this page\n",
    "    table = soup.find_all(\"tr\", attrs={\"id\": \"row_\"})  # Get list of games on this page\n",
    "    df = pd.DataFrame(columns=[\"id\", \"name\", \"nrate\", \"pic_url\"], index=range(len(table)))  # DF to hold this pages results\n",
    "    \n",
    "    # Loop through each row and pull out the info for that game\n",
    "    for idx, row in enumerate(table):\n",
    "        # Row may or may not start with a \"boardgame rank\" link, if YES then strip it\n",
    "        links = row.find_all(\"a\")\n",
    "        if \"name\" in links[0].attrs.keys():\n",
    "            del links[0]\n",
    "        gamelink = links[1]  # Get the relative URL for the specific game\n",
    "        gameid = int(gamelink[\"href\"].split(\"/\")[2])  # Get the game ID by parsing the relative URL\n",
    "        gamename = gamelink.contents[0]  # Get the actual name of the game as the link contents\n",
    "        imlink = links[0]  # Get the URL for the game thumbnail\n",
    "        thumbnail = imlink.contents[0][\"src\"]\n",
    "\n",
    "        ratings_str = row.find_all(\"td\", attrs={\"class\": \"collection_bggrating\"})[2].contents[0]\n",
    "        nratings = int(\"\".join(ratings_str.split()))\n",
    "\n",
    "        df.iloc[idx, :] = [gameid, gamename, nratings, thumbnail]\n",
    "\n",
    "    # Concatenate the results of this page to the master dataframe\n",
    "    min_nrate = df[\"nrate\"].min()  # The smallest number of ratings of any game on the page\n",
    "    print(\"Page %i scraped, minimum number of ratings was %i\" % (npage, min_nrate))\n",
    "    df_all = pd.concat([df_all, df], axis=0)\n",
    "    npage += 1\n",
    "    sleep(2) # Keep the BGG server happy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all.copy()\n",
    "# Reset the index since we concatenated a bunch of DFs with the same index into one DF\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "# Write the DF to .csv for future use\n",
    "df.to_csv(\"games_list.csv\", index=False, encoding=\"utf-8\")\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
